{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFUVE4K8e9oE"
      },
      "source": [
        "#Multi-Armed Bandit\n",
        "In this exercise session we will consider the UCB1 and TS algorithms as examples of frequentist\n",
        "and Bayesian MAB algorithms, respectively, for the solutions of the stochastic\n",
        "MAB problem.\n",
        "\n",
        "Let us instantiate a stochastic MAB environment with 5 arms with Bernoulli reward:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEnW7t0ae7V0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Set the environment\n",
        "exp_reward = np.array([0.15, 0.1, 0.1, 0.2, 0.35, 0.2])\n",
        "n_arms = len(exp_reward)\n",
        "opt = np.max(exp_reward)\n",
        "idx_opt = np.argmax(exp_reward)\n",
        "deltas = opt - exp_reward\n",
        "deltas = np.array([delta for delta in deltas if delta > 0])\n",
        "\n",
        "#Time horizon\n",
        "T = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fS5ozxsh0_A"
      },
      "source": [
        "Recall that in this example we are aware of the environment, while in real-world applications we do not have the model of the environment, therefore selecting the optimal arm is not a viable operation.\n",
        "\n",
        "Instead, we want to resort to an online approach to minimize the loss due to the learning of the optimal arm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olNpHo7gbnyX"
      },
      "source": [
        "## UCB1\n",
        "As a first algorithm we will implement the UCB1 algorithm, which uses **upper confidence bounds** to select the arm to pull for the next round.\n",
        "\n",
        "We recap here the main steps of the algorithm:\n",
        "- For each time step $t$\n",
        "- Compute $\\hat{R}_t(a_l) = \\frac{\\sum_{j = 1}^{t-1} r_{a_{i_j},j} \\ \\boldsymbol{1} \\left\\{ a_{i_j} = a_{l} \\right\\}}{N_t(a_l) } \\ \\ \\forall a_l$\n",
        "- Compute $B_t(a_l) = \\sqrt{\\frac{2\\log t}{N_t(a_l)}} \\ \\ \\forall a_l$\n",
        "- Play arm $a_{i_t} = \\arg \\max_{a_l \\in \\mathcal{A}} \\left\\{ \\hat{R}_t(a_l) + B_t(a_l) \\right\\}$\n",
        "\n",
        "After the execution of the algorithm we would like to evaluate its performances.\n",
        "In this\n",
        "case, as usual in the MAB literature, we resort to the concept of expected pseudo-regret:\n",
        "$$L_T = T R^* - \\mathbb{E} \\left[ \\sum_{t = 1}^T R(a_{i_t}) \\right] = \\sum_{a_i \\in \\mathcal{A} : \\Delta_i > 0} \\mathbb{E}[N_T(a_i)] \\Delta_i$$\n",
        "\n",
        "A first approximated version of the regret can be computed as follows:\n",
        "$$\\tilde{L}_T = \\sum_{t = 1}^T [r_{a^*}-r_{a_{i_t}}]$$\n",
        "\n",
        "\n",
        "Let's see how it can be implemented:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDfYCYkTbzSo"
      },
      "source": [
        "def UCB1():\n",
        "  ucb1_criterion = np.zeros(n_arms)\n",
        "  expected_payoffs = np.zeros(n_arms)\n",
        "  number_of_pulls = np.zeros(n_arms)\n",
        "\n",
        "  regret = np.array([])\n",
        "  pseudo_regret = np.array([])\n",
        "\n",
        "  for t in range(T):\n",
        "      #Select an arm\n",
        "      if t < n_arms:\n",
        "        pulled_arm = t  # round robin for the first n steps\n",
        "      else:\n",
        "        idxs = np.argwhere(ucb1_criterion == ucb1_criterion.max()).reshape(-1)  # there can be more arms with max value\n",
        "        pulled_arm = np.random.choice(idxs) #arbitrary\n",
        "\n",
        "      #Pull an arm\n",
        "      reward = np.random.binomial(1, exp_reward[pulled_arm]) #Bernoulli reward\n",
        "      if pulled_arm != idx_opt:\n",
        "        reward_opt = np.random.binomial(1, exp_reward[idx_opt])\n",
        "      else:\n",
        "        reward_opt = reward\n",
        "\n",
        "      #Update UCB1\n",
        "      number_of_pulls[pulled_arm] = number_of_pulls[pulled_arm] + 1\n",
        "      expected_payoffs[pulled_arm] = ((expected_payoffs[pulled_arm] * (number_of_pulls[pulled_arm] - 1.0) + reward) /\n",
        "                                      number_of_pulls[pulled_arm]) # update sample mean for the selected arm\n",
        "      if t >= n_arms:\n",
        "        for k in range(0, n_arms):\n",
        "          ucb1_criterion[k] = expected_payoffs[k] + np.sqrt(2 * np.log(t) / number_of_pulls[k])\n",
        "\n",
        "      #Store results\n",
        "      regret = np.append(regret, reward_opt - reward)\n",
        "      pseudo_regret = np.append(pseudo_regret, opt - exp_reward[pulled_arm])\n",
        "  return regret, pseudo_regret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxAVrDEq46z7"
      },
      "source": [
        "regret, pseudo_regret = UCB1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnKtbQk05MBp"
      },
      "source": [
        "An example of the execution of the UCB1 algorithm is available at:\n",
        "https://drive.google.com/file/d/1OMDBpbvCldsXdnBnnXGZIoZNDfchHoj1/view?usp=sharing\n",
        "\n",
        "Here we compare the approximated regret with the expected pseudo-regret we obtained with a single run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1qTw1eydAk6"
      },
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.ylabel(\"Regret\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.plot(np.cumsum(pseudo_regret), color='r', label='Pseudo-regret')\n",
        "plt.plot(np.cumsum(regret), color='g', label='Regret')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIuT4hRq5zmq"
      },
      "source": [
        "Notice that the expected pseudo-regret is monotonically increasing over time, and, conversely, the regret might also decrease.\n",
        "\n",
        "However, to evaluate the performance of our approach in the correct way we also have to average between multiple runs, in order to take into account the  of the environment randomness. To evalate the uncertainty of the estimates, we also add the $95\\%$ confidence intervals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5SCqYb81igX"
      },
      "source": [
        "n_repetitions = 5\n",
        "regrets, pseudo_regrets = np.zeros((n_repetitions, T)), np.zeros((n_repetitions, T))\n",
        "for i in range(n_repetitions):\n",
        "  regrets[i], pseudo_regrets[i] = UCB1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VS-VOeg9HXM"
      },
      "source": [
        "# Compute the cumulative sum\n",
        "cumu_regret = np.cumsum(regrets, axis=1)\n",
        "cumu_pseudo_regret = np.cumsum(pseudo_regrets, axis=1)\n",
        "\n",
        "# Take the average over different runs\n",
        "avg_cumu_regret = np.mean(cumu_regret, axis=0)\n",
        "avg_cumu_pseudo_regret = np.mean(cumu_pseudo_regret, axis=0)\n",
        "\n",
        "std_cumu_regret = np.std(cumu_regret, axis=0)\n",
        "std_cumu_pseudo_regret = np.std(cumu_pseudo_regret, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RliqWpt7_Kqb"
      },
      "source": [
        "We also want to understand wheter we are respecting the UCB1 upper bound:\n",
        "\t$$L_T \\leq  8 \\log T \\sum_{a_i \\in \\mathcal{A}: \\Delta_i > 0} \\frac{1}{\\Delta_i} + \\left(1 + \\frac{\\pi^2}{3} \\right) \\sum_{a_i \\in \\mathcal{A}: \\Delta_i > 0} \\Delta_i$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YMgqH9-_KWp"
      },
      "source": [
        "ucb1_upper_bound = np.array([8*np.log(t)*sum(1/deltas) + (1 + np.pi**2/3)*sum(deltas)\n",
        "                             for t in range(1,T+1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUVv7BGI9y4L"
      },
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.ylabel(\"Regret\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.plot(avg_cumu_pseudo_regret, color='r', label='Pseudo-regret')\n",
        "\n",
        "plt.plot(avg_cumu_regret + 1.96 * std_cumu_regret / np.sqrt(n_repetitions), linestyle='--', color='g')\n",
        "plt.plot(avg_cumu_regret, color='g', label='Regret')\n",
        "plt.plot(avg_cumu_regret - 1.96 * std_cumu_regret / np.sqrt(n_repetitions), linestyle='--', color='g')\n",
        "plt.plot(ucb1_upper_bound, color='b', label='Upper bound')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoxqOpBRa-4B"
      },
      "source": [
        "##Thompson Sampling\n",
        "In this second example we are resorting to a Bayesian method.\n",
        "For each arm we use a uniform distribution as prior for the expected value of the reward, i.e., $\\mu_i \\sim Beta(1, 1) = U([0, 1])$.\n",
        "During the learning process we update the beta distribution using the posterior provided by the reward:\n",
        "*   if we observe a success: we update the $\\alpha$ parameter of the Beta increasing its value by one\n",
        "*   if we observe a failure: we update the $\\beta$ parameter of the Beta increasing its value by one\n",
        "\n",
        "Formally:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUKgo1xGa-LX"
      },
      "source": [
        "def thompson_sampling():\n",
        "  T = 5000\n",
        "  beta_parameters = np.ones((n_arms,2))\n",
        "\n",
        "  regret = np.array([])\n",
        "  pseudo_regret = np.array([])\n",
        "\n",
        "  for t in range(1,T+1):\n",
        "      #Select an arm\n",
        "      samples = np.random.beta(beta_parameters[:,0], beta_parameters[:,1])\n",
        "      pulled_arm =  np.argmax(samples)\n",
        "\n",
        "      #Pull an arm\n",
        "      reward = np.random.binomial(1, exp_reward[pulled_arm]) #Bernoulli reward\n",
        "      if pulled_arm != idx_opt:\n",
        "        reward_opt = np.random.binomial(1, exp_reward[idx_opt])\n",
        "      else:\n",
        "        reward_opt = reward\n",
        "\n",
        "      #Update TS\n",
        "      beta_parameters[pulled_arm,0] =  beta_parameters[pulled_arm,0] + reward\n",
        "      beta_parameters[pulled_arm,1] =  beta_parameters[pulled_arm,1] + 1.0 - reward\n",
        "\n",
        "      #Store results\n",
        "      regret = np.append(regret, reward_opt - reward)\n",
        "      pseudo_regret = np.append(pseudo_regret, opt - exp_reward[pulled_arm])\n",
        "  return regret, pseudo_regret, beta_parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwZ2V3i4Q_F2"
      },
      "source": [
        "As before, we start analysing the single run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkPRlGVSMpW3"
      },
      "source": [
        "regret, pseudo_regret, beta_parameters = thompson_sampling()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A84mIrk2KqTV"
      },
      "source": [
        "print(beta_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8MVRBvFjkKg"
      },
      "source": [
        "Notice how the parameters for the arms with larger expected rewards have larger values, meaning that during the learning process we selected those arms more often than the others.\n",
        "\n",
        "An example of the execution of the TS algorithm is available at:\n",
        "https://drive.google.com/file/d/1CC1YVM06-AEWQ71dXxoZPNBDUZpjeWSo/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZfwqXCZZDYK"
      },
      "source": [
        "from scipy.stats import beta\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16, 9))\n",
        "x = np.linspace(0,1,1000)\n",
        "colors = ['red', 'green', 'blue', 'purple', 'orange']\n",
        "for params, rew, color in zip(beta_parameters, exp_reward, colors):\n",
        "  rv = beta(*params)\n",
        "  plt.plot(x, rv.pdf(x), color=color)\n",
        "  plt.axvline(rew, linestyle='--', color=color)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39veoCQYY0dt"
      },
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.ylabel(\"Regret\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.plot(np.cumsum(pseudo_regret), label='Pseudo-regret', color='r')\n",
        "plt.plot(np.cumsum(regret), label='Regret', color='g')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c0bAHSiLOcw"
      },
      "source": [
        "n_repetitions = 5\n",
        "regrets, pseudo_regrets = np.zeros((n_repetitions, T)), np.zeros((n_repetitions, T))\n",
        "for i in range(n_repetitions):\n",
        "  regrets[i], pseudo_regrets[i], _ = thompson_sampling()\n",
        "\n",
        "# Compute the cumulative sum\n",
        "cumu_regret = np.cumsum(regrets, axis=1)\n",
        "cumu_pseudo_regret = np.cumsum(pseudo_regrets, axis=1)\n",
        "\n",
        "# Take the average over different runs\n",
        "avg_cumu_regret = np.mean(cumu_regret, axis=0)\n",
        "avg_cumu_pseudo_regret = np.mean(cumu_pseudo_regret, axis=0)\n",
        "\n",
        "std_cumu_regret = np.std(cumu_regret, axis=0)\n",
        "std_cumu_pseudo_regret = np.std(cumu_pseudo_regret, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA2Zmba9NEPV"
      },
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "plt.ylabel(\"Regret\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.plot(avg_cumu_pseudo_regret, color='r', label='pseudo-regret')\n",
        "plt.plot(avg_cumu_regret + std_cumu_regret, linestyle='--', color='g')\n",
        "plt.plot(avg_cumu_regret, color='g', label='regret')\n",
        "plt.plot(avg_cumu_regret - std_cumu_regret, linestyle='--', color='g')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxQgb-esTvf_"
      },
      "source": [
        "# Homeworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l55qP4oIijzP"
      },
      "source": [
        "##MAB Lower bound\n",
        "\n",
        "Visualize the regret Lower bound for stochastic MAB problems.\n",
        "Check if there exists a Lower bound which does not requires to know the distribution, i.e., that does not require to know the arms' gaps $\\Delta_i$ and visualize it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In2Pdk1Qj254"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL1vzAinTfAX"
      },
      "source": [
        "## Thompson Sampling Upper Bound\n",
        "Visualize the pseudo regret and the Upper bound for the considered problem provided by TS algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51RPBcLQTukg"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKFtLvzVTg9L"
      },
      "source": [
        "## Adversarial MAB\n",
        "\n",
        "Model an adversarial MAB environment with $5$ arms and $\\{0,1\\}$ rewards. A suggestion is to consider an adversary which decides the rewards for all the rounds in advance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iZ1ItvrTnta"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R16gB4UEToah"
      },
      "source": [
        "## EXP3\n",
        "\n",
        "Implement the EXP3 algorithm for adversarial bandit defined in the MAB setting defined before. Compute the expected regret and compare it with the theoretical upper and lower bounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYEbjPELTtc0"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}