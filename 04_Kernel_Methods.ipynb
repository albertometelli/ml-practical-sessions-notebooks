{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhTO_S4vcNqp"
      },
      "source": [
        "# Kernel Methods\n",
        "During this exercise session we will analyse the use of kernel methods to perform regression and classification tasks.\n",
        "At first, we use Gaussian process to compute the value of the petal width given the petal length in the Iris dataset. After that, we use SVMs to classify the setosa and non-setosa flowers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk02nitYGCCn"
      },
      "source": [
        "## Gaussian Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI1SXUhOOdMN"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "from sklearn import datasets\n",
        "\n",
        "dataset = datasets.load_iris()\n",
        "\n",
        "x = zscore(dataset.data[:, 2]).reshape(-1, 1) # column 2 of data is petal length\n",
        "t = zscore(dataset.data[:, 3]) # column 3 of data is petal width"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSNGCUW2PG7y"
      },
      "source": [
        "Once we have our dataset, we would like to set up a GP for regression\n",
        "\n",
        "At first we set up our kernel. In this case we choose a standard Gaussian kernel, a.k.a. RBF kernel:\n",
        "$$K(\\mathbf{x}, \\mathbf{y}) : = \\phi \\exp \\left\\{ - \\frac{||\\mathbf{x} - \\mathbf{y}||_2^2}{2 l^2} \\right\\}$$\n",
        "\n",
        "Moreover, we have to choose the value of the variance $\\sigma^2$ of the noise of the data we are considering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cxxGrOOPNOZ"
      },
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
        "\n",
        "#Kernel values\n",
        "phi = 0.7\n",
        "l = 0.9\n",
        "sigma_sq = 0.02\n",
        "kernel = ConstantKernel(phi, constant_value_bounds=\"fixed\") * RBF(l, length_scale_bounds=\"fixed\")\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, alpha=sigma_sq).fit(x, t)\n",
        "x_pred = np.linspace(np.min(x),np.max(x),5000).reshape(-1,1)\n",
        "t_pred, sigma = gpr.predict(x_pred, return_std=True)\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(x, t, 'r.', markersize=10, label='Observations')\n",
        "plt.plot(x_pred, t_pred, 'b-', label='Prediction')\n",
        "plt.fill(np.concatenate([x_pred, x_pred[::-1]]),\n",
        "         np.concatenate([t_pred - 1.9600 * sigma,\n",
        "                        (t_pred + 1.9600 * sigma)[::-1]]),\n",
        "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
        "plt.xlabel('$x$', fontsize=18)\n",
        "plt.ylabel('$t$', fontsize=18)\n",
        "plt.legend(loc='upper left', fontsize=18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KxLchrqbZQt"
      },
      "source": [
        "Changing the values of the kernel parameters influences the dynamic of the GP\n",
        "For instance if:\n",
        "- set $l = 8$ we increase the smoothness of the GP\n",
        "- set $\\sigma^2 = 10$ we increase the noise in each point\n",
        "- set $\\phi = 100$ we increase the influence of the kernel to the results\n",
        "\n",
        "\n",
        "To optimize these parameters before starting the regression procedure, we can use either an independent dataset or cross-validation to estimate them, maximizing the likelihood of the considered samples. In what follows, we will use the points from the Iris dataset to optimize the parameter $l$ and $\\phi$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = ConstantKernel(phi, constant_value_bounds=\"fixed\") * RBF(0.1, length_scale_bounds=\"fixed\")\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, alpha=sigma_sq).fit(x, t)\n",
        "x_pred = np.linspace(np.min(x),np.max(x),5000).reshape(-1,1)\n",
        "t_pred, sigma = gpr.predict(x_pred, return_std=True)\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.plot(x, t, 'r.', markersize=10, label='Observations')\n",
        "plt.plot(x_pred, t_pred, 'b-', label='Prediction')\n",
        "plt.fill(np.concatenate([x_pred, x_pred[::-1]]),\n",
        "         np.concatenate([t_pred - 1.9600 * sigma,\n",
        "                        (t_pred + 1.9600 * sigma)[::-1]]),\n",
        "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
        "plt.xlabel('$x$', fontsize=18)\n",
        "plt.ylabel('$t$', fontsize=18)\n",
        "plt.legend(loc='upper left', fontsize=18)"
      ],
      "metadata": {
        "id": "Omp0AgRDvoXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpr.kernel_"
      ],
      "metadata": {
        "id": "bG-X81pxwtTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR90jFOIOjpa"
      },
      "source": [
        "Finally, a useful resource for GPs https://thegradient.pub/gaussian-process-not-quite-for-dummies/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJJ7CnfmmGRw"
      },
      "source": [
        "## SVM\n",
        "The application of kernel methods to classification is easily exemplified in the SVMs\n",
        "\n",
        "We start with the linear formulation, for which we are applying a linear kernel $K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^\\top \\mathbf{y}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVZ7ZcdyU10v"
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "input = zscore(dataset.data[:, [0, 1]]) # column 0,1 are sepal length and sepal width\n",
        "\n",
        "# the dataset is stored in the variable iris\n",
        "target = dataset.target.copy()\n",
        "# 0 - setosa, 1 - versicolor, 2 - virginica\n",
        "target[target == 1] = 2\n",
        "target[target == 0] = 1\n",
        "target[target == 2] = 0\n",
        "\n",
        "SVM_model = svm.SVC(kernel='linear')\n",
        "SVM_model.fit(input, target)\n",
        "\n",
        "#check the support vectors\n",
        "SVM_model.support_vectors_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLhdvRpexBFD"
      },
      "source": [
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "plt.scatter(input[:, 0], input[:, 1], c=target, s=30, cmap=plt.cm.Paired)\n",
        "\n",
        "# plot the decision function\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "\n",
        "# create grid to evaluate model\n",
        "xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "#evaluate the SVM value for the positive class\n",
        "Z = SVM_model.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "# plot decision boundary (w^Tx =  0) and margins (w^Tx = 1 and w^Tx = -1)\n",
        "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
        "           linestyles=['--', '-', '--'])\n",
        "# plot support vectors\n",
        "ax.scatter(SVM_model.support_vectors_[:, 0], SVM_model.support_vectors_[:, 1], s=100,\n",
        "           linewidth=1, facecolors='none', edgecolors='k')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPwoLXqafE0B"
      },
      "source": [
        "Instead, if we want to add a Gaussian kernel (or RBF):\n",
        "$$ K(\\mathbf{x}, \\mathbf{y}) = \\exp \\left\\{ - \\frac{||\\mathbf{x} - \\mathbf{y}||^2}{l} \\right\\} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfnyDN6wZyX9"
      },
      "source": [
        "SVM_model = svm.SVC() # default kernel is 'rbf'\n",
        "#default scale coefficient (gamma=\"scale\", corresponding to 1/l) is a heuristic value\n",
        "SVM_model.fit(input, target)\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "plt.scatter(input[:, 0], input[:, 1], c=target, s=30, cmap=plt.cm.Paired)\n",
        "\n",
        "# plot the decision function\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "\n",
        "# create grid to evaluate model\n",
        "xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "#evaluate the SVM value for the positive class\n",
        "Z = SVM_model.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "# plot decision boundary (w^Tx =  0) and margins (w^Tx = 1 and w^Tx = -1)\n",
        "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
        "           linestyles=['--', '-', '--'])\n",
        "# plot support vectors\n",
        "ax.scatter(SVM_model.support_vectors_[:, 0], SVM_model.support_vectors_[:, 1], s=100,\n",
        "           linewidth=1, facecolors='none', edgecolors='k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxm519pRarhQ"
      },
      "source": [
        "The margins are not linear anymore. We still have the support vectors (How can we identify them?) which are those data that are used to provide a prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zmEdAuaU-xN"
      },
      "source": [
        "## Homeworks\n",
        "\n",
        "Here we propose some exercises in python for you. They are not mandatory, but they can be helpful to better understand the contents of the lecture, by giving you the opportunity to develop some code by yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OYJOnfhVCTF"
      },
      "source": [
        "1) Predicting petal width\n",
        "\n",
        "Consider again the Iris dataset, and write a code to predict the petal width by using, this time, all the other features as input. Use as predictor the GP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ecKso-VLP5"
      },
      "source": [
        "2) Using GP for classification\n",
        "\n",
        "Instead use GP as a classification tool to predict setosa vs. non-setosa flowers. Use the appropriate techniques to determinte the performance of this methods we might have on newly seen data. Is the mean squared error a good performance metric even under the probabilistic model provided by GPs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-yKE9O9Y78n"
      },
      "source": [
        "3) Implementing SVM for multiple outputs\n",
        "\n",
        "Extend the SVM to predict the three classes present in the Iris Dataset. Do we need to apply a specific methods to handle the three classes?\n",
        "Compare the results obtained with the ones provided by other classification methods presented during the course."
      ]
    }
  ]
}